import filecmp
import os
import copy
import logging
import time

import requests
import docker
import ruamel.yaml as yaml
import json
import jinja2

import utils
from abstracts import base_adaptor as abco
from abstracts.exceptions import AdaptorCritical
from toscaparser.tosca_template import ToscaTemplate
from toscaparser.functions import GetProperty

logger = logging.getLogger("adaptor." + __name__)


class TerraformDict(dict):
    def __init__(self):
        super().__init__()
        self["//"] = "This file has been generated by the MiCADO Terraform Adaptor"
        self.resource = []
        self.data = []
        self.provider = {}
        self.variable = {}
        self.command_variables = ""

    def add_provider(self, name, properties):
        self.setdefault("provider", {})
        self["provider"][name] = properties
        self.provider = self["provider"]

    def add_variable(self, name, properties={}):
        self.variable[name] = properties

    def add_resource(self, name, resource):
        self.setdefault("resource", {})
        self["resource"].setdefault(name, [])
        if resource not in self["resource"][name]:
            self["resource"][name].append(resource)
        self.resource = self["resource"]

    def add_data(self, name, data):
        self.setdefault("data", {})
        self["data"].setdefault(name, [])
        if data not in self["data"][name]:
            self["data"][name].append(data)
        self.data = self["data"]

    def add_count_variable(self, name, value):
        self.add_variable(name, {"default": value})
    
    def update_counts(self, old_counts):
        new_counts = {}
        for count_var, default in self.variable.items():
            if count_var in old_counts:
                new_counts[count_var] = old_counts[count_var]
            else:
                new_counts[count_var] = default
        self.variable = new_counts

    def dump_json(self, path_to_tf, path_to_vars):
        utils.dump_json(self, path_to_tf)
        utils.dump_json(self.variable, path_to_vars)


class TerraformAdaptor(abco.Adaptor):
    def __init__(self, adaptor_id, config, dryrun, validate=False, template=None):
        """
        Constructor method of the Adaptor
        """
        super().__init__()
        if template and not isinstance(template, ToscaTemplate):
            raise AdaptorCritical("Template is not a valid TOSCAParser object")
        self.status = "init"
        self.dryrun = dryrun
        self.volume = config["volume"]
        self.validate = validate
        self.node_name = ""
        self.min_instances = 1
        self.max_instances = 1
        self.ID = adaptor_id
        self.template = template

        self.cloud_init_template = "./system/cloud_init_worker_tf.yaml"

        self.tf_file = "{}{}.tf.json".format(self.volume, self.ID)
        self.tf_file_tmp = "{}{}.tf.json.tmp".format(self.volume, self.ID)
        self.vars_file = "{}terraform.tfvars.json".format(self.volume)
        self.vars_file_tmp = "{}terraform.tfvars.json.tmp".format(self.volume)
        self.account_file = "{}accounts.json".format(self.volume)

        self.terra_data = {}
        # TODO delete terra_data

        self.tf_json = TerraformDict()

        self.created = False
        self.terraform = None
        self.cloud_inits = []
        if not self.dryrun:
            self._init_docker()

        self.auth_data_file = "/var/lib/submitter/system/auth_data.yaml"
        self.auth_gce = "/var/lib/submitter/system/accounts.json"
        self.master_cert = "/var/lib/submitter/system/master.pem"
        self.terra_gce = "/var/lib/submitter/system/gce_lvm.tf"
        self.terra_path = "/var/lib/micado/terraform/submitter/"
        logger.info("Terraform adaptor initialised")

    def translate(self, update=False):
        """
        Translate the self.tpl subset to Terraform node infrastructure format
        This fuction creates a mapping between TOSCA and Terraform template descriptor.
        """
        logger.info("Starting Terraform Translation")
        self.status = "translating..."
        self.tf_json = TerraformDict()

        for node in self.template.nodetemplates:

            if "_" in node.name:
                raise AdaptorCritical(
                    "Underscores in node {} not allowed".format(node.name)
                )
            self.node_name = node.name
            self.terra_data = {}

            node = copy.deepcopy(node)
            cloud_type = self._node_data_get_interface(node)
            if not cloud_type:
                continue

            self._get_policies(node)

            properties = self._get_properties_values(node)
            context = properties.get("context")
            cloud_init = self._node_data_get_context_section(context)
            self.cloud_inits.append(cloud_init)

            if cloud_type == "ec2":
                logger.info("EC2 resource detected")
                aws_properties = self._node_data_get_ec2_host_properties(node)
                self._add_terraform_aws(aws_properties)
            elif cloud_type == "nova":
                logger.info("Nova resource detected")
                self._node_data_get_nova_host_properties(node, "resource")
            elif cloud_type == "azure":
                logger.info("Azure resource detected")
                self._add_terraform_azure(properties)
            elif cloud_type == "gce":
                logger.info("GCE resource detected")
                self._node_data_get_gce_host_properties(node, "resource")

        if not self.tf_json.provider:
            logger.info("No nodes to orchestrate with Terraform. Skipping...")
            self.status = "Skipped"
            return

        if update:
            logger.info("Creating temp files")
            old_count_vars = utils.load_json(self.vars_file)
            self.tf_json.update_counts(old_count_vars)
            self.tf_json.dump_json(self.tf_file_tmp, self.vars_file_tmp)

        elif not self.validate:
            self.tf_json.dump_json(self.tf_file, self.vars_file)
            self._rename_cloud_inits()

        self.status = "Translated"

    def execute(self):
        """
        Initialize terraform execution environment and execute
        """
        logger.info("Starting Terraform execution {}".format(self.ID))
        self.status = "executing"
        if not self._config_file_exists():
            logger.info("No config generated during translation, nothing to execute")
            self.status = "Skipped"
            return
        elif self.dryrun:
            logger.info("DRY-RUN: Terraform execution in dry-run mode...")
            self.status = "DRY-RUN Deployment"
            return
        else:
            if self.created:
                logger.debug("Terraform initialization starting...")
                result = self.terraform.exec_run(
                    "terraform init", workdir="{}".format(self.terra_path)
                )
                logger.debug("Terraform initialization has been successful")
                logger.debug(result)

                logger.debug("Terraform build starting...")
                exit_code, out = self.terraform.exec_run(
                    "terraform apply -auto-approve",
                    workdir="{}".format(self.terra_path),
                )

                if exit_code == 1:
                    raise AdaptorCritical(out)
                logger.debug("Terraform build has been successful")

            else:
                logger.error("Terraform deployment was unsuccessfull!")
                raise AdaptorCritical("Terraform deployment was unsuccessful!")
        logger.info("Terraform executed")
        self.status = "executed"

    def undeploy(self):
        """
        Undeploy Terraform infrastructure
        """
        self.status = "undeploying"
        logger.info("Undeploying {} infrastructure".format(self.ID))
        if not self._config_file_exists():
            logger.info("No config generated during translation, nothing to undeploy")
            self.status = "Skipped"
            return
        elif self.dryrun:
            logger.info("DRY-RUN: deleting infrastructure...")
        else:

            self.terraform.exec_run(
                "terraform destroy -lock=false -auto-approve",
                workdir="{}".format(self.terra_path),
            )
        self.status = "undeployed"

    def destroy_selected(self, vm_name):
        """
        Destroy a specific virtual machine
        """
        self.status = "updating"
        logger.info("Updating the infrastructure {}".format(self.ID))
        self.terraform.exec_run(
            "terraform destroy -target={0} -lock=false -auto-approve",
            workdir="{}".format(self.terra_path),
        )

    def cleanup(self):
        """
        Remove the generated files under "files/output_configs/"
        """
        logger.info("Cleanup config for ID {}".format(self.ID))
        if not self._config_file_exists():
            logger.info("No config generated during translation, nothing to cleanup")
            self.status = "Skipped"
            return
        try:
            os.remove(self.tf_file)
        except OSError as e:
            logger.warning(e)

        self._remove_cloud_inits()

        # Deletion of files used only in GCE
        # TODO delete this?
        try:
            os.remove(self.vars_file)
        except OSError:
            pass
        try:
            os.remove(self.account_file)
        except OSError:
            pass

    def update(self):
        """
        Check that if it's any change in the node definition or in the cloud-init file.
        If the node definition changed then rerun the build process. If the infrastructure definition
        changed first undeploy the infrastructure and rebuild it with the modified parameter.
        """
        self.status = "updating"
        self.min_instances = 1
        self.max_instances = 1
        logger.info("Updating the infrastructure {}".format(self.ID))
        self.translate(update=True)

        if not self.tf_json.provider and os.path.exists(self.tf_file):
            logger.debug("All Terraform nodes removed from ADT. Undeploying...")
            self._remove_tmp_files
            self.undeploy()
            self.cleanup()
            self.status = "Updated - undeployed"

        elif not self.tf_json.provider:
            logger.debug("No Terraform nodes added to ADT")
            self._remove_tmp_files
            self.status = "Skipped"

        elif self._differentiate(self.tf_file, self.tf_file_tmp):
            logger.debug("Terraform file changed, replacing and executing...")
            os.rename(self.tf_file_tmp, self.tf_file)
            os.rename(self.vars_file_tmp, self.vars_file)
            self._rename_cloud_inits
            self.execute()
            self.status = "Updated Terraform file"

        elif self._differentiate_cloud_inits():
            logger.debug("Cloud-init file(s) changed, replacing old executing")
            os.rename(self.tf_file_tmp, self.tf_file)
            os.rename(self.vars_file_tmp, self.vars_file)
            self._rename_cloud_inits
            self.execute()
            self.status = "Updated cloud_init files"

        else:
            self.status = "Updated (nothing to update)"
            logger.info("There are no changes in the Terraform files")
            self._remove_tmp_files

    def _node_data_get_interface(self, node):
        """
        Get cloud relevant information from tosca
        """
        interfaces = utils.get_lifecycle(node, "Terraform")
        if not interfaces:
            logger.debug("No interface for Terraform in {}".format(node.name))
            return None
        cloud_inputs = interfaces.get("create")

        # Resolve get_property in interfaces
        for field, value in cloud_inputs.items():
            if isinstance(value, GetProperty):
                cloud_inputs[field] = value.result()
                continue
            elif not isinstance(value, dict) or not "get_property" in value:
                continue
            cloud_inputs[field] = node.get_property_value(value.get("get_property")[-1])

        return cloud_inputs["provider"]

    def _node_data_get_context_section(self, context):
        """
        Create the cloud-init config file
        """
        if not context:
            logger.info("The adaptor will use a default cloud-config")
            node_init = self._get_cloud_init(None, False, False)

        elif context.get("append"):
            if not context.get("cloud_config"):
                logger.info(
                    "You set append properties but you do not have cloud_config. Please check it again!"
                )
                raise AdaptorCritical(
                    "You set append properties but you don't have cloud_config. Please check it again!"
                )
            else:
                logger.info("Append the TOSCA cloud-config to the default config")
                node_init = self._get_cloud_init(context["cloud_config"], True, False)

        else:
            if not context.get("cloud_config"):
                logger.info("The adaptor will use a default cloud-config")
                node_init = self._get_cloud_init(None, False, False)
            else:
                logger.info("The adaptor will use the TOSCA cloud-config")
                node_init = self._get_cloud_init(context["cloud_config"], False, True)

        cloud_init_file_name = "{}-cloud-init.yaml".format(self.node_name)
        cloud_init_path = "{}{}".format(self.volume, cloud_init_file_name)
        cloud_init_path_tmp = "{}.tmp".format(cloud_init_path)

        utils.dump_order_yaml(node_init, cloud_init_path_tmp)
        return cloud_init_path

    def _node_data_get_ec2_host_properties(self, properties):
        """
        Get EC2 properties and create node definition
        """
        aws_properties = {}
        aws_properties["region"] = properties["region_name"]
        aws_properties["ami"] = properties["image_id"]
        aws_properties["instance_type"] = properties["instance_type"]
        if properties.get("key_name"):
            aws_properties["key_name"] = properties["key_name"]
        if properties.get("security_group_ids"):
            security_groups = properties["security_group_ids"]
            aws_properties["vpc_security_group_ids"] = security_groups

        return aws_properties

    def _node_data_get_azure_host_properties(self, node):
        """
        Get Azure properties and create node definition
        """
        pass

    def _node_data_get_gce_host_properties(self, node, key):
        """
        Get GCE properties and create node definition
        """
        properties = self._get_properties_values(node)
        self.terra_data.setdefault("region", properties["region"])
        self.terra_data.setdefault("project", properties["project"])
        self.terra_data.setdefault("machine_type", properties["machine_type"])
        self.terra_data.setdefault("zone", properties["zone"])
        self.terra_data.setdefault("image", properties["image"])
        self.terra_data.setdefault("network", properties["network"])
        self._node_data_get_context_section(properties)
        if properties.get("ssh-keys") is not None:
            self.terra_data.setdefault("ssh-keys", properties["ssh-keys"])

    def _node_data_get_nova_host_properties(self, node, key):
        """
        Get NOVA properties and create node definition
        """
        properties = self._get_properties_values(node)

        self.terra_data.setdefault("image_id", properties["image_id"])
        self.terra_data.setdefault("flavor_id", properties["flavor_id"])
        self.terra_data.setdefault("auth_url", properties["auth_url"])
        self.terra_data.setdefault("tenant_id", properties["project_id"])
        self.terra_data.setdefault("network_name", properties["network_name"])
        self.terra_data.setdefault("network_id", properties["network_id"])
        self.terra_data.setdefault("key_pair", properties["key_name"])
        self._node_data_get_context_section(properties)
        if properties.get("security_groups") is not None:
            security_groups = list()
            security_groups = properties["security_groups"]
            self.terra_data.setdefault("security_groups", security_groups[0])

    def _get_cloud_init(self, tosca_cloud_config, append, override):
        """
        Get cloud-config from MiCADO cloud-init template
        """
        yaml.default_flow_style = False
        default_cloud_config = {}
        with open(self.master_cert, "r") as p:
            master_file = p.read()
        try:
            with open(self.cloud_init_template, "r") as f:
                template = jinja2.Template(f.read())
                rendered = template.render(
                    worker_name=self.node_name, master_pem=master_file
                )
                default_cloud_config = yaml.round_trip_load(
                    rendered, preserve_quotes=True
                )
        except OSError as e:
            logger.error(e)
        if override:
            return yaml.round_trip_load(tosca_cloud_config, preserve_quotes=True)
        if tosca_cloud_config is not None:
            tosca_cloud_config = yaml.round_trip_load(
                tosca_cloud_config, preserve_quotes=True
            )
        if append:
            for x in default_cloud_config:
                for y in tosca_cloud_config:
                    if x == y:
                        for z in tosca_cloud_config[y]:
                            default_cloud_config[x].append(z)
            return default_cloud_config
        else:
            return default_cloud_config

    def _init_docker(self):
        """ Initialize docker and get Terraform container """
        client = docker.from_env()
        i = 0

        while not self.created and i < 5:
            try:
                self.terraform = client.containers.list(
                    filters={"label": "io.kubernetes.container.name=terraform"}
                )[0]
                self.created = True
            except Exception as e:
                i += 1
                logger.error("{0}. Try {1} of 5.".format(str(e), i))
                time.sleep(5)

    def _get_properties_values(self, node):
        """ Get host properties """
        return {x: y.value for x, y in node.get_properties().items()}

    def _get_policies(self, node):
        """ Get the TOSCA policies """
        self.min_instances = 1
        self.max_instances = 1
        if "scalable" in node.entity_tpl.get("capabilities", {}):
            scalable = node.get_capabilities()["scalable"]
            self.min_instances = scalable.get_property_value("min_instances")
            self.max_instances = scalable.get_property_value("max_instances")
            return
        for policy in self.template.policies:
            for target in policy.targets_list:
                if self.node_name == target.name:
                    logger.debug("policy target match for compute node")
                    properties = self._get_properties_values(policy)
                    self.min_instances = properties["min_instances"]
                    self.max_instances = properties["max_instances"]

    def _differentiate(self, path, tmp_path):
        """ Compare two files """
        return not filecmp.cmp(path, tmp_path)

    def _differentiate_cloud_inits(self):
        """ Compare cloud inits """
        for cloud_init in self.cloud_inits:
            cloud_init_tmp = "{}.tmp".format(cloud_init)
            if os.path.exists(cloud_init) and self._differentiate(
                cloud_init, cloud_init_tmp
            ):
                return True

    def _add_terraform_aws(self, properties):
        """ Add Terraform template for AWS to JSON"""

        # Get the credentials info
        credential = self._get_credential_info("aws")

        # Add the provider info
        region = properties.pop("region")
        aws_region = self.tf_json.provider.get("aws", {}).get("region")
        if aws_region and aws_region != region:
            raise AdaptorCritical("Multiple different AWS regions is unsupported")
        elif not aws_region:
            aws_provider = {
                "region": region,
                "access_key": credential["accesskey"],
                "secret_key": credential["secretkey"],
            }
            self.tf_json.add_provider("aws", aws_provider)

        # Add the count variable
        instance_name = self.node_name
        count_var_name = "{}-count".format(instance_name)
        self.tf_json.add_count_variable(count_var_name, self.min_instances)

        # Add the resource
        aws_instance = {
            instance_name: {
                **properties,
                "user_data": '${file("${path.module}/terrainit.yaml")}',
                "count": "${var.%s}" % count_var_name,
            }
        }
        self.tf_json.add_resource("aws_instance", aws_instance)

    def _write_tera_nova(self):
        """ Write Terraform template files for openstack"""
        credential = self._get_credential_info("nova")

        f = open(self.tf_file_tmp, "w+")
        f.write('variable "x" {\n')
        f.write('  default = "1"\n')
        f.write("}\n")
        f.write("\n")
        f.write('provider "openstack" {\n')
        f.write('  auth_url = "%s"\n' % (self.terra_data["auth_url"]))
        f.write('  tenant_id = "%s"\n' % (self.terra_data["tenant_id"]))
        f.write('  user_name = "%s"\n' % (credential["username"]))
        f.write('  password = "%s"\n' % (credential["password"]))
        f.write("}\n")
        f.write("\n")
        f.write(
            'resource "openstack_compute_instance_v2" "%s" {\n'
            % (self.terra_data["name"])
        )
        f.write('  name = "%s ${count.index}"\n' % (self.terra_data["name"]))
        f.write('  image_id = "%s"\n' % (self.terra_data["image_id"]))
        f.write('  flavor_id = "%s"\n' % (self.terra_data["flavor_id"]))
        f.write('  key_pair = "%s"\n' % (self.terra_data["key_pair"]))
        f.write('  security_groups = ["%s"]\n' % (self.terra_data["security_groups"]))
        f.write('  user_data = "${file("${path.module}/terrainit.yaml")}"\n')
        f.write("  count = var.x\n")
        f.write("\n")
        f.write("  network {\n")
        f.write('    name = "%s"\n' % (self.terra_data["network_name"]))
        f.write('    uuid = "%s"\n' % (self.terra_data["network_id"]))
        f.write("  }\n")
        f.write("}\n")
        f.close()

    def _add_terraform_azure(self, properties):
        """ Write Terraform template files for Azure"""

        def get_provider():
            return {
                "subscription_id": properties["subscription_id"],
                "client_id": credential["client_id"],
                "client_secret": credential["client_secret"],
                "tenant_id": properties["tenant_id"],
            }

        def get_resource_group():
            return {resource_group_name: {"name": resource_group_name}}

        def get_virtual_network():
            return {
                virtual_network_name: {
                    "name": virtual_network_name,
                    "resource_group_name": "${data.azurerm_resource_group.%s.name}"
                    % resource_group_name,
                }
            }

        def get_subnet():
            return {
                subnet_name: {
                    "name": subnet_name,
                    "resource_group_name": "${data.azurerm_resource_group.%s.name}"
                    % resource_group_name,
                    "virtual_network_name": "${data.azurerm_virtual_network.%s.name}"
                    % virtual_network_name,
                }
            }

        def get_network_security_group():
            return {
                network_security_group_name: {
                    "name": network_security_group_name,
                    "resource_group_name": "${data.azurerm_resource_group.%s.name}"
                    % resource_group_name,
                }
            }

        def get_network_interface():
            return {
                network_interface_name: {
                    "name": "%s${count.index}" % network_interface_name,
                    "location": "${data.azurerm_resource_group.%s.location}"
                    % resource_group_name,
                    "resource_group_name": "${data.azurerm_resource_group.%s.name}"
                    % resource_group_name,
                    "network_security_group_id": "${data.azurerm_network_security_group.%s.id}"
                    % network_security_group_name,
                    "count": "${var.%s}" % count_var_name,
                    "ip_configuration": {
                        "name": "%s${count.index}" % nic_config_name,
                        "subnet_id": "${data.azurerm_subnet.%s.id}" % subnet_name,
                        "private_ip_address_allocation": "Dynamic",
                    },
                }
            }

        def get_virtual_machine():
            return {
                instance_name: {
                    "name": "%s${count.index}" % instance_name,
                    "location": "${data.azurerm_resource_group.%s.location}"
                    % resource_group_name,
                    "resource_group_name": "${data.azurerm_resource_group.%s.name}"
                    % resource_group_name,
                    "network_interface_ids": [
                        "${element(azurerm_network_interface.%s.*.id, count.index)}"
                        % network_interface_name
                    ],
                    "vm_size": virtual_machine_size,
                    "count": "${var.%s}" % count_var_name,
                    "delete_os_disk_on_termination": "true",
                    "delete_data_disks_on_termination": "true",
                    "storage_os_disk": {
                        "name": "%s${count.index}" % virtual_machine_disk_name,
                        "caching": "ReadWrite",
                        "create_option": "FromImage",
                        "managed_disk_type": "Standard_LRS",
                    },
                    "storage_image_reference": {
                        "publisher": "Canonical",
                        "offer": "UbuntuServer",
                        "sku": virtual_machine_image,
                        "version": "latest",
                    },
                    "os_profile": {
                        "computer_name": "micado-worker",
                        "admin_username": "ubuntu",
                        "custom_data": '${file("${path.module}/%s")}'
                        % cloud_init_file_name,
                    },
                    "os_profile_linux_config": {
                        "disable_password_authentication": "true",
                        "ssh_keys": {
                            "path": "/home/ubuntu/.ssh/authorized_keys",
                            "key_data": ssh_key_data,
                        },
                    },
                }
            }

        # Begin building the JSON

        instance_name = self.node_name

        count_var_name = "{}-count".format(instance_name)
        self.tf_json.add_count_variable(count_var_name, self.min_instances)

        credential = self._get_credential_info("azure")
        self.tf_json.add_provider("azurerm", get_provider())

        resource_group_name = properties["rg_name"]
        self.tf_json.add_data("azurerm_resource_group", get_resource_group())

        virtual_network_name = properties["vn_name"]
        self.tf_json.add_data("azurerm_virtual_network", get_virtual_network())

        subnet_name = properties["sn_name"]
        self.tf_json.add_data("azurerm_subnet", get_subnet())

        network_security_group_name = properties["nw_sec_group"]
        self.tf_json.add_data(
            "azurerm_network_security_group", get_network_security_group()
        )

        nic_config_name = "{}-nic-config".format(instance_name)
        network_interface_name = "{}-nic".format(instance_name)
        self.tf_json.add_resource("azurerm_network_interface", get_network_interface())

        virtual_machine_size = properties["vm_size"]
        virtual_machine_disk_name = "{}-disk".format(instance_name)
        virtual_machine_image = properties["image"]
        cloud_init_file_name = "{}-cloud-init.yaml".format(instance_name)
        ssh_key_data = properties.get("key_data", "")

        self.tf_json.add_resource("azurerm_virtual_machine", get_virtual_machine())

    def _write_tera_gce(self):
        """ Write Terraform template files for GCE"""
        f = open(self.vars_file_tmp, "w+")
        f.write('vm_name = "%s"\n' % (self.terra_data["name"]))
        f.write('region = "%s"\n' % (self.terra_data["region"]))
        f.write('project = "%s"\n' % (self.terra_data["project"]))
        f.write('machine_type = "%s"\n' % (self.terra_data["machine_type"]))
        f.write('zone = "%s"\n' % (self.terra_data["zone"]))
        f.write('image = "%s"\n' % (self.terra_data["image"]))
        f.write('network = "%s"\n' % (self.terra_data["network"]))
        if self.terra_data.get("ssh-keys") is not None:
            f.write('ssh-keys = "%s"\n' % (self.terra_data["ssh-keys"]))

        with open(self.terra_gce) as p:
            with open(self.tf_file_tmp, "w+") as p1:
                for line in p:
                    p1.write(line)

        with open(self.auth_gce) as q:
            with open(self.account_file, "w+") as q1:
                for line in q:
                    q1.write(line)

    def _config_file_exists(self):
        """ Check if config file was generated during translation """
        return os.path.exists(self.tf_file)

    def _remove_tmp_files(self):
        """ Remove tmp files generated by the update step """
        try:
            os.remove(self.tf_file_tmp)
            logger.debug("File deleted: {}".format(self.tf_file_tmp))
        except OSError:
            pass

        try:
            os.remove(self.vars_file_tmp)
            logger.debug("File deleted: {}".format(self.vars_file_tmp))
        except OSError:
            pass

        for cloud_init in self.cloud_inits:
            cloud_init_tmp = cloud_init + ".tmp"
            try:
                os.remove(cloud_init_tmp)
                logger.debug("File deleted: {}".format(cloud_init_tmp))
            except OSError:
                pass

    def _remove_cloud_inits(self):
        """ Remove cloud_init files on undeploy """
        for file in os.listdir(self.volume):
            if "cloud-init.yaml" in file:
                try:
                    os.remove(file)
                except OSError:
                    pass

    def _rename_cloud_inits(self):
        """ Rename temporary cloud_init files """
        for cloud_init in self.cloud_inits:
            cloud_init_tmp = cloud_init + ".tmp"
            os.rename(cloud_init_tmp, cloud_init)

    def _get_credential_info(self, provider):
        """ Return credential info from file """
        with open(self.auth_data_file, "r") as stream:
            temp = yaml.safe_load(stream)
        resources = temp.get("resource", {})
        for resource in resources:
            if resource.get("type") == provider:
                return resource.get("auth_data")

